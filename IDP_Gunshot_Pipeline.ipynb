{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piyush-c38/Gunshot_Direction_Estimation/blob/main/IDP_Gunshot_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete the workflow pipeline"
      ],
      "metadata": {
        "id": "yBwsMA-NIEPd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXGsR1AC49Pj"
      },
      "outputs": [],
      "source": [
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import scipy.signal as signal\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import joblib\n",
        "import os\n",
        "import noisereduce as nr\n",
        "from multiprocessing import Process, Manager\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# === Constants ===\n",
        "DURATION = 5  # seconds\n",
        "FS = 44100\n",
        "LOWCUT = 3000\n",
        "HIGHCUT = 15000\n",
        "GAIN_DB = 6\n",
        "MIC_INDICES = [5, 6, 7]\n",
        "MIC_LABELS = [\"mic1\", \"mic2\", \"mic3\"]\n",
        "TEST_ANGLE = \"test\"\n",
        "\n",
        "# === Model Paths ===\n",
        "LABEL_ENCODER_PATH = \"./trained_models/label_encoder_510.pkl\"\n",
        "MODEL_1_PATH = \"./trained_models/laptop_vggish_trained_model_510data.keras\"\n",
        "MODEL_2_PATH = './trained_models/gunshot_direction_model_v2.pkl'\n",
        "\n",
        "# === Load ML Model 1 ===\n",
        "model_1 = tf.keras.models.load_model(MODEL_1_PATH)\n",
        "label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
        "vggish_model = hub.load('https://tfhub.dev/google/vggish/1')\n",
        "\n",
        "# === Load ML Model 2 ===\n",
        "model_2, model_2_features = joblib.load(MODEL_2_PATH)\n",
        "\n",
        "# === VGGish Embedding Extractor ===\n",
        "def extract_vggish_embeddings(audio, sr=16000):\n",
        "    if len(audio) < int(sr * 0.96):\n",
        "        raise ValueError(\"Audio too short for VGGish.\")\n",
        "    desired_len = sr * 10\n",
        "    if len(audio) < desired_len:\n",
        "        audio = np.pad(audio, (0, desired_len - len(audio)), mode='constant')\n",
        "    else:\n",
        "        audio = audio[:desired_len]\n",
        "    audio_tensor = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
        "    embeddings = vggish_model(audio_tensor)\n",
        "    return np.mean(embeddings.numpy(), axis=0)\n",
        "\n",
        "# === Predict Gunshot / Not ===\n",
        "def predict_audio_array(audio, sr):\n",
        "    audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
        "    embedding = extract_vggish_embeddings(audio)\n",
        "    embedding = np.expand_dims(embedding, axis=0)\n",
        "    prediction = model_1.predict(embedding)\n",
        "    predicted_index = np.argmax(prediction)\n",
        "    predicted_label = label_encoder.inverse_transform([predicted_index])[0]\n",
        "    return predicted_label\n",
        "\n",
        "# === Predict Direction ===\n",
        "def predict_direction(m1, m2, m3):\n",
        "    diff12 = m1 - m2\n",
        "    diff13 = m1 - m3\n",
        "    diff23 = m2 - m3\n",
        "    total = m1 + m2 + m3\n",
        "    norm1, norm2, norm3 = m1 / total, m2 / total, m3 / total\n",
        "    peak_intensity = max(m1, m2, m3)\n",
        "    features = np.array([[m1, m2, m3, diff12, diff13, diff23, norm1, norm2, norm3, peak_intensity]])\n",
        "    return model_2.predict(features)[0]\n",
        "\n",
        "# === Process Each Mic Audio ===\n",
        "# === Process Each Mic Audio ===\n",
        "def process_audio(audio, fs, label, results_dict, test_number):\n",
        "    # Save raw audio\n",
        "    raw_audio = audio.copy()\n",
        "\n",
        "    # Bandpass Filter\n",
        "    order = 4\n",
        "    nyq = 0.5 * fs\n",
        "    low = LOWCUT / nyq\n",
        "    high = HIGHCUT / nyq\n",
        "    b, a = signal.butter(order, [low, high], btype='band')\n",
        "    filtered_audio = signal.filtfilt(b, a, audio)\n",
        "\n",
        "    # Noise Reduction\n",
        "    denoised_audio = nr.reduce_noise(y=filtered_audio, sr=fs)\n",
        "\n",
        "    # Save raw and filtered audios\n",
        "    sf.write(f\"{TEST_ANGLE}_{test_number}_{label}_raw.wav\", raw_audio, fs)\n",
        "    sf.write(f\"{TEST_ANGLE}_{test_number}_{label}_filtered.wav\", denoised_audio, fs)\n",
        "\n",
        "    # Find peak from filtered audio\n",
        "    t = np.linspace(0, len(audio)/fs, len(audio))\n",
        "    pos_idx = np.where(denoised_audio > 0)[0]\n",
        "    if len(pos_idx) > 0:\n",
        "        max_index = np.argmax(denoised_audio[pos_idx])\n",
        "        peak_val = denoised_audio[pos_idx[max_index]]\n",
        "        peak_time = t[pos_idx[max_index]]\n",
        "        print(f\"üîç {label.upper()} Peak: {peak_val:.4f} at {peak_time:.4f}s\")\n",
        "    else:\n",
        "        peak_val = 0\n",
        "        print(f\"‚ö†Ô∏è {label.upper()} No positive peak detected.\")\n",
        "        peak_time = None\n",
        "\n",
        "    # Save both audios and peak to results dict\n",
        "    results_dict[label] = {\n",
        "        \"raw_audio\": raw_audio,          # For gunshot classification\n",
        "        \"filtered_audio\": denoised_audio, # For peak detection\n",
        "        \"sr\": fs,\n",
        "        \"peak\": peak_val\n",
        "    }\n",
        "\n",
        "# === Record Audio from Mic ===\n",
        "def record_mic(index, label, results_dict, test_number):\n",
        "    print(f\"üéôÔ∏è Recording {label}...\")\n",
        "    audio = sd.rec(int(DURATION * FS), samplerate=FS, channels=1, dtype='float32', device=index)\n",
        "    sd.wait()\n",
        "    audio = audio.flatten()\n",
        "    process_audio(audio, FS, label, results_dict, test_number)\n",
        "\n",
        "#=== Plot Radar ===\n",
        "def plot_gunshot_direction(angle):\n",
        "\n",
        "    if(angle == 0 or angle == 360):\n",
        "        start_angle = -30\n",
        "        end_angle = 30\n",
        "    elif(angle == 60):\n",
        "        start_angle = 270\n",
        "        end_angle = 330\n",
        "    elif(angle == 120):\n",
        "        start_angle = 210\n",
        "        end_angle = 270\n",
        "    elif(angle == 180):\n",
        "        start_angle = 150\n",
        "        end_angle = 210\n",
        "    elif(angle == 240):\n",
        "        start_angle = 90\n",
        "        end_angle = 150\n",
        "    elif(angle == 300):\n",
        "        start_angle = 30\n",
        "        end_angle = 90\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw={'projection': 'polar'})\n",
        "    fig.patch.set_facecolor('black')  # Background of figure\n",
        "\n",
        "    # Radar settings\n",
        "    ax.set_facecolor('darkgreen')    # Radar (circle) background\n",
        "    ax.set_theta_zero_location('N')  # 0 degrees at the top\n",
        "    ax.set_theta_direction(-1)       # Clockwise\n",
        "    ax.set_rticks([])                # Remove radial ticks\n",
        "    ax.set_xticks(np.deg2rad(np.arange(0, 360, 60)))  # Add gridlines at every 45¬∞\n",
        "    ax.grid(color='lightgreen', linestyle='--', linewidth=0.7)\n",
        "\n",
        "    # Plot detected sector\n",
        "    theta = np.linspace(np.deg2rad(start_angle), np.deg2rad(end_angle), 100)\n",
        "    r = np.ones_like(theta)\n",
        "    ax.fill_between(theta, 0, r, color='lime', alpha=0.6)  # Highlighted detected region\n",
        "\n",
        "    # Circle border\n",
        "    circle = plt.Circle((0, 0), 1, transform=ax.transData._b, color='lightgreen', fill=False, linewidth=2)\n",
        "    ax.add_artist(circle)\n",
        "    plt.show()\n",
        "\n",
        "# === Main Pipeline ===\n",
        "def main():\n",
        "    test_number = input(\"Enter test number: \")\n",
        "\n",
        "    print(\"üé¨ Starting 3-mic recording session...\")\n",
        "    with Manager() as manager:\n",
        "        results = manager.dict()\n",
        "        processes = []\n",
        "\n",
        "        for idx, label in zip(MIC_INDICES, MIC_LABELS):\n",
        "            p = Process(target=record_mic, args=(idx, label, results, test_number))\n",
        "            p.start()\n",
        "            processes.append(p)\n",
        "        for p in processes:\n",
        "            p.join()\n",
        "\n",
        "        # Extract peak values and audios\n",
        "        mic_peaks = {label: results[label][\"peak\"] for label in MIC_LABELS}\n",
        "        mic_raw_audio = {label: results[label][\"raw_audio\"] for label in MIC_LABELS}\n",
        "        mic_sr = results[MIC_LABELS[0]][\"sr\"]\n",
        "\n",
        "        # Find mic with max peak\n",
        "        max_mic_label = max(mic_peaks, key=mic_peaks.get)\n",
        "        selected_raw_audio = mic_raw_audio[max_mic_label]\n",
        "\n",
        "        print(f\"\\nüìå Highest peak from: {max_mic_label.upper()}\")\n",
        "\n",
        "        # Predict gunshot using raw audio\n",
        "        predicted_label = predict_audio_array(selected_raw_audio, mic_sr)\n",
        "\n",
        "        if predicted_label.lower() == \"gunshot\":\n",
        "            print(\"‚úÖ Gunshot Detected!\")\n",
        "            # Predict direction using filtered peaks\n",
        "            direction = predict_direction(mic_peaks[\"mic1\"], mic_peaks[\"mic2\"], mic_peaks[\"mic3\"])\n",
        "            print(f\"üéØ Predicted Direction: {direction}¬∞\")\n",
        "            plot_gunshot_direction(direction)\n",
        "        else:\n",
        "            print(\"‚ùå Not a gunshot.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}